# Defining topological priors to constrain learning-based image segmentation

*Semester project within the ResProj unit, Spring 2025*

*Rim Faroudy, under the supervision of Pr. Maria Zuluaga and PhD students Vincenzo Marcianò and Luisa Vargas.*

## Introduction

The purpose of this 100 hours project was to study the role of topological constraints in guiding a machine learning model towards a more accurate segmentation for medical images. In our specific case, we used diffusion models and brain MRIs.

We implemented a pipeline that is trained on brain MRI images and their ground truth segmentation, and is able to generate topologically correct synthetic MRI images. First, a  ControlNet is trained to generate MRI images constrained by their ground truth segmentation. Then, a diffusion model is trained with a topological loss to generate plausible segmentation masks. Finally, we use the masks generated by the diffusion model as constraints to infer on the ControlNet and generate new MRI images.

This can serve the purpose of data augmentation in a field where real data is scarce due to how expensive it is to collect. It could be used to train a segmentation model.

In order to fully understand the pipeline, it is important to be familiar with diffusion models and have notions of persistent homology. A presentation about the basics of Topological Data Analysis is present in this repository (`TDA_presentation_28032025.pdf`). The final presentation for the project is also present, just in case (`final_presentation_26062025.pdf`).

## Resources

The main data set used is the BraTS Task 01 Dataset.

The main libraries used throughout this project were `PyTorch` and `MONAI`.

To build the pipeline, we referred to the ControlNet and TopoInteraction repositories.

### BraTS

The Brain Tumor Segmentation Challenge (BraTS) is an international competition focused on automating segmentation of brain tumors from multi-modal MRI scans.

For each patient, a T1 weighted (T1w), a post-contrast enhanced T1-weighted (T1CE), a T2-weighted (T2w) and a Fluid-Attenuated Inversion Recovery (FLAIR) MRI is provided.

Each patient also has an associated segmentation mask that identifies the main tumor subregions. These masks are 3D volumes where each voxel is labeled : 1 for the necrotic and non-enhancing core (NCR/NET), 2 for the edema (ED), and 4 for the enhancing tumor (ET). Areas without tumor are labeled as 0.

> Links :
> 
> 
> [https://www.med.upenn.edu/cbica/brats/](https://www.med.upenn.edu/cbica/brats/)
> 
> [https://www.kaggle.com/datasets/aryashah2k/brain-tumor-segmentation-brats-2019](https://www.kaggle.com/datasets/aryashah2k/brain-tumor-segmentation-brats-2019)
> 

### MONAI

MONAI (Medical Open Network for AI) is an open-source framework built on top of PyTorch, designed specifically for deep learning in medical imaging. It provides tools for data loading, preprocessing, model training, evaluation, and deployment, with support for multi-modal imaging and 3D data.

> Main website :
> 
> 
> [https://github.com/project-monai/monai](https://github.com/project-monai/monai)
> 
> Tutorials :
> 
> [https://github.com/Project-MONAI/tutorials](https://github.com/Project-MONAI/tutorials)
> 

### ControlNet

ControlNets are hypernetworks that allow for supplying extra conditioning to ready-trained diffusion models. They were introduced in *Adding Conditional Control to Text-to-Image Diffusion Models* by Zhang et al.

> MONAI tutorial for ControlNet in 2D :
> 
> 
> [https://github.com/Project-MONAI/tutorials/blob/main/generation/controlnet/2d_controlnet.ipynb](https://github.com/Project-MONAI/tutorials/blob/main/generation/controlnet/2d_controlnet.ipynb)
> 
> ControlNet paper :
> 
> [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)
> 

### TI_loss

The Topological Interaction loss was introduced in Learning *Topological Interactions for Multi-Class
Medical Image Segmentation* by Gupta et al. and is available on the paper’s repository.

> TopoInteraction repository :
> 
> 
> [https://github.com/TopoXLab/TopoInteraction/blob/main/TI_Loss.py](https://github.com/TopoXLab/TopoInteraction/blob/main/TI_Loss.py)
> 
> TopoInteraction paper :
> 
> [https://arxiv.org/pdf/2207.09654](https://arxiv.org/pdf/2207.09654)
> 

## Environment setup

To run this pipeline locally, it’s better to first create and activate a virtual environment that already has Python. Then, install all the (necessary and optional) dependencies. The versions we worked with can be installed with :

```bash
# PyTorch 2.3.1 with CUDA 12.1
pip install torch==2.3.1+cu121 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121

# MONAI stable (1.6.x), with all optional dependencies
pip install monai[all]==1.6.0
```

<aside>
⚠️

We modified the `lib\python3.11\site-packages\monai\inferers\inferer.py` file to handle returning both the prediction AND the denoised image. The prediction is used for the ControlNet and the denoised image is used for the TI loss.

</aside>

But it’s better to continually check the `MONAI` repository linked above for the most accurate versioning, especially to avoid conflicts with `torch`.

The following dependencies are also necessary : 

```bash
pip install matplotlib numpy cripser tqdm
```

It is necessary to replace the existing file with the one provided in this repository.

## Pipeline

The pipeline can be synthesized into 3 main parts : the data loading and preprocessing, the ControlNet, and the diffusion model.

In practice, it is ordered as follows :

1. Load the data (images and labels containing the 4 class segmentation).
2. Train a ControlNet to generate MRI images based on segmentation masks
(using the BraTS ground truth data)
3. Train a Diffusion Model with the TI loss to generate topologically accurate
segmentation masks from noise (using the BraTS ground truth data)
4. Infer on the Diffusion Model to generate synthetic segmentation masks
5. Infer on the ControlNet to generate MRI images based on the
segmentation masks generated in step 3

### Data loading, splitting and preprocessing

The data is imported using MONAI. If it is already present in the environment, it is not downloaded again. If it is absent, it is downloaded.

It then goes through the same transformations as are described on the ControlNet tutorial repository by MONAI, without transforming the labels into a single binary mask (since we are trying to segment using those labels and constrain on their interactions in the topological interaction loss).

It is then split as follows :

- Same training set for the base diffusion model for ControlNet, the ControlNet itself, and the diffusion model with TI loss ;
- Same validation set for the base diffusion model for ControlNet and the ControlNet itself ;
- Separate validation set for the diffusion model with TI loss

Split counts:
- Train (same for ControlNet and TopoDiffNet): 310
- ControlNet val : 48
- TopoDiffNet val : 48
- Test (for the whole pipeline): 78

### ControlNet : training

The first part trains a 2D diffusion model based on a U-Net architecture (`DiffusionModelUNet`). 

During training, the model uses a MSE loss. This is implemented using MONAI’s `DiffusionInferer`. Periodically, the model generates images, and these samples are visualized to track training progress.

At the end of training, the model weights and optimizer state are saved to disk as a checkpoint.

The second part trains a ControlNet model that conditions image generation on segmentation masks. The ControlNet is initialized by copying the weights from the previously trained diffusion model, while freezing the base model’s parameters to train only the ControlNet.

Training uses the same MSE loss framework and MONAI’s `ControlNetDiffusionInferer` to incorporate conditioning during both training and sampling. Periodically, the ControlNet generates images conditioned on segmentation masks, which are visualized to track progress.

At the end of training, the ControlNet weights and optimizer state are saved to disk as a checkpoint.

We used 100 epochs of training for the base diffusion model and 150 epochs for the ControlNet.

### Diffusion model with topological loss : training and inference

<aside>
⚠️

REMINDER

We modified the `lib\python3.11\site-packages\monai\inferers\inferer.py` file to handle returning both the prediction AND the denoised image. The prediction is used for the ControlNet and the denoised image is used for the TI loss.

</aside>

This part trains a 2D diffusion model based on a U-Net architecture (`DiffusionModelUNet` again) with 4 input and output channels, corresponding to one-hot encoded segmentation masks.

The loss combines the standard MSE noise prediction loss with a Topological Interaction (TI) loss that penalizes topological errors in the predicted segmentations. The TI loss is computed by extracting discrete segmentation maps (via argmax over softmax outputs) and comparing them to integer ground truth labels.

The TI module identifies critical voxels where topological constraints are violated based on specified inclusion and exclusion rules between classes, and uses these to weight a cross-entropy loss locally.

We use the denoised image instead of the predicted noise because the TI loss requires a discrete segmentation map to evaluate topological correctness. The denoised image is an estimate of the clean segmentation at the current timestep, so after applying argmax it gives a meaningful predicted label map. In contrast, the predicted noise represents residual noise, which is not suitable for computing topology-based losses that depend on discrete class assignments.

At the end, model weights and optimizer state are saved as a checkpoint.

We used 100 epochs for training.

### ControlNet : inference

The noise tensor `noise = torch.randn((1, 4, 64, 64)).to(device)` is used as the initial input for the TopoDiffNet inference. This noise matches the number of classes (4 channels) because the diffusion model operates in this multi-class space.

The inference step `seg = inferer.sample(...)` generates a predicted segmentation by denoising the noise input with the diffusion model with TI loss. The output is converted to a single label using `argmax` to get a segmentation map usable by the ControlNet.

A separate noise tensor `noise_controlnet = torch.randn((1, 1, 64, 64)).float().to(device)` is created for the ControlNet model, which generates MRI images conditioned on the segmentation mask generated earlier.

The ControlNet inference samples an MRI image by taking this noise input, the pretrained base diffusion model, the ControlNet model, and the generated segmentation mask (`seg`) as conditioning (`cn_cond`).

Finally, both the generated segmentation mask and the corresponding synthetic MRI image are displayed side by side for comparison and qualitative assessment. An additional block highlights high-contrast regions in a generated MRI using the Laplacian filter, then visualizes these areas alongside the original MRI and its segmentation label for qualitative analysis.

## Continuing our work

Limitations of this work include :

- the high computational cost of training diffusion models with topological losses, with only 100–200 epochs completed
- sensitivity to hyperparameters like the fixed TI loss weight (1e-5) without tuning
- limited scalability to 3D or higher-resolution images since training was done at 64×64 resolution

Future work could involve : 

- adding position-based constraints to focus tumor generation
- generalizing to 3D data
- implementing evaluation metrics such as SSIM and PSNR for MRI quality and Dice and HD95 for segmentation masks
- and training segmentation models using the generated data
